{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from ast import literal_eval\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read each fitting results file and print the fitting criteria and mean parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../../data/fitting_results\"\n",
    "FIGURE_DIR = \"../../figures\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent type: lqr, n=111\n",
      "lqr llh: -12814.370781999998\n",
      "lqr AIC: 26072.741563999996\n",
      "lqr llh mean: -115.44478181981981\n",
      "lqr AIC mean: 234.88956363963962\n",
      "mean exp param: 0.009413725503555946\n",
      "mean vm param: 2.0164565486785424\n",
      "agent type: sparse_lqr, n=111\n",
      "sparse_lqr llh: -8980.059542\n",
      "sparse_lqr AIC: 18626.119084\n",
      "sparse_lqr llh mean: -80.9014373153153\n",
      "sparse_lqr AIC mean: 167.80287463063055\n",
      "agent type: sparse_max_discrete, n=111\n",
      "sparse_max_discrete llh: -8200.462375\n",
      "sparse_max_discrete AIC: 17288.92475\n",
      "sparse_max_discrete llh mean: -73.8780394144144\n",
      "sparse_max_discrete AIC mean: 155.7560788288288\n",
      "agent type: sparse_max_continuous, n=111\n",
      "sparse_max_continuous llh: -8415.976799000002\n",
      "sparse_max_continuous AIC: 17719.953598000004\n",
      "sparse_max_continuous llh mean: -75.8196108018018\n",
      "sparse_max_continuous AIC mean: 159.6392216036036\n",
      "agent type: null_model_1, n=111\n",
      "null_model_1 llh: -8873.0970816\n",
      "null_model_1 AIC: 18634.1941632\n",
      "null_model_1 llh mean: -79.93781154594593\n",
      "null_model_1 AIC mean: 167.87562309189187\n",
      "agent type: null_model_2, n=111\n",
      "null_model_2 llh: -8720.581285\n",
      "null_model_2 AIC: 17885.16257\n",
      "null_model_2 llh mean: -78.56379536036036\n",
      "null_model_2 AIC mean: 161.12759072072072\n",
      "agent type: hill_climbing, n=111\n",
      "hill_climbing llh: -8310.943554\n",
      "hill_climbing AIC: 17287.887108\n",
      "hill_climbing llh mean: -74.87336535135137\n",
      "hill_climbing AIC mean: 155.7467307027027\n"
     ]
    }
   ],
   "source": [
    "# create a dictionary to store dataframes\n",
    "all_model_dfs = {}\n",
    "for agent_type in ['lqr', 'sparse_lqr', 'sparse_max_discrete', 'sparse_max_continuous', 'null_model_1', 'null_model_2', 'hill_climbing']:\n",
    "    # read the aggregated fitting results\n",
    "    df = pd.read_csv(f\"{DATA_DIR}/fitting_results_{agent_type}.csv\")\n",
    "    all_model_dfs[agent_type] = df\n",
    "    # print info about \n",
    "    print(f\"agent type: {agent_type}, n={len(df)}\")\n",
    "    print(f\"{agent_type} llh: {df['ll'].sum()}\")\n",
    "    print(f\"{agent_type} AIC: {df['AIC'].sum()}\")\n",
    "    print(f\"{agent_type} llh mean: {df['ll'].mean()}\")\n",
    "    print(f\"{agent_type} AIC mean: {df['AIC'].mean()}\")\n",
    "    if agent_type == \"lqr\":\n",
    "        print(f\"mean exp param: {df['exp_param'].mean()}\")\n",
    "        print(f\"mean vm param: {df['vm_param'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the participant ids\n",
    "pp_nrs = pd.read_csv('../../data/experimental_data/experiment_ppids.csv')['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the number of participants best fit by each model, as well as the strength of evidence for the best model over the second-best model for each participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'null_model_2': 17, 'hill_climbing': 37, 'sparse_max_discrete': 33, 'sparse_lqr': 12, 'sparse_max_continuous': 11, 'lqr': 1})\n"
     ]
    }
   ],
   "source": [
    "models_by_best_fitting_pps = defaultdict(int)\n",
    "participant_to_best_model = {}\n",
    "\n",
    "for pp_id in pp_nrs:\n",
    "    participant_fits = {}\n",
    "    for agent_type in ['lqr', 'sparse_lqr', 'sparse_max_discrete', 'sparse_max_continuous', 'null_model_1', 'null_model_2', 'hill_climbing']:\n",
    "        # select the dataframe for the selected agent type\n",
    "        df = all_model_dfs[agent_type]\n",
    "\n",
    "        if len(df[df['pp_id'] == pp_id]['AIC']) != 1:\n",
    "            print(\"len:\", len(df[df['pp_id'] == pp_id]['AIC']))\n",
    "            break\n",
    "        \n",
    "        # add the AIC to the participant fits dictionary\n",
    "        participant_fits[agent_type] = float(df[df['pp_id'] == pp_id]['AIC'])\n",
    "    \n",
    "    if len(df[df['pp_id'] == pp_id]) != 1:\n",
    "        continue\n",
    "    \n",
    "    # select the best-fitting agent for this participant\n",
    "    sorted_fits = sorted(participant_fits.values())\n",
    "    best_agent = min(participant_fits, key=participant_fits.get)\n",
    "    \n",
    "    # increment the number of pps best fit by the model\n",
    "    models_by_best_fitting_pps[best_agent] += 1\n",
    "    # store the best-fitting model for this participant\n",
    "    participant_to_best_model[pp_id] = best_agent\n",
    "\n",
    "print(models_by_best_fitting_pps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create csv files for Bayesian model selection (done using SPM8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aics = pd.DataFrame()\n",
    "for df_type in all_model_dfs:\n",
    "    # convert AICs to log model evidence format\n",
    "    df_aics[df_type] = all_model_dfs[df_type]['AIC'].apply(lambda x: -x/2)\n",
    "df_aics.to_csv(f\"{DATA_DIR}/aic_lme.csv\")  # save to csv\n",
    "\n",
    "n_params = {\"null_model_2\": 2, \"null_model_1\": 4, \"lqr\": 2, \"sparse_lqr\": 3, \"hill_climbing\": 3, \"sparse_max_continuous\": 4, \"sparse_max_discrete\": 4}\n",
    "\n",
    "df_bics = pd.DataFrame()\n",
    "for df_type in all_model_dfs:\n",
    "    # convert BICs to log model evidence format\n",
    "    df_bics[df_type] = all_model_dfs[df_type][\"ll\"].apply(lambda x: - (n_params[df_type] * np.log(10) - 2 * x) / 2)\n",
    "df_bics.to_csv(f\"{DATA_DIR}/bic_lme.csv\")  # save to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a csv file with the best-fitting model and parameters for each participant\n",
    "df_bestfit = pd.DataFrame()\n",
    "for pp_id in pp_nrs:\n",
    "    best_model_type = participant_to_best_model[pp_id]  # pick the best model type\n",
    "    df = all_model_dfs[best_model_type]  # get the dataframe for the best model\n",
    "    row = df[df['pp_id'] == pp_id]  # get the relevant row using the pp id\n",
    "    df_bestfit = df_bestfit.append(row, ignore_index=True)  # add the selected row to the best fit dataframe\n",
    "df_bestfit.to_csv(f\"{DATA_DIR}/best_fitting_models.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the mean best-fitting parameter for each model type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL TYPE: null_model_2\n",
      "exp param: 0.02945709633235715\n",
      "vm param: 5.549289613974336\n",
      "MODEL TYPE: hill_climbing\n",
      "exp param: 0.06982937336977825\n",
      "vm param: 5.850748535172354\n",
      "step size: 0.38101831604162145\n",
      "MODEL TYPE: sparse_max_discrete\n",
      "exp param: 0.08594615322697012\n",
      "vm param: 4.1322034045095934\n",
      "step size: 0.6517082678364109\n",
      "attention cost: 14.843517028179017\n",
      "MODEL TYPE: sparse_lqr\n",
      "exp param: 0.04818076964280734\n",
      "vm param: 4.6945031869458385\n",
      "attention cost: 135.88634989998812\n",
      "MODEL TYPE: sparse_max_continuous\n",
      "exp param: 0.05239728619664927\n",
      "vm param: 5.334390566884107\n",
      "step size: 0.20743542412651053\n",
      "attention cost: 13.66615299113521\n",
      "MODEL TYPE: lqr\n",
      "exp param: 0.0247858565556271\n",
      "vm param: 4.218178722788898\n"
     ]
    }
   ],
   "source": [
    "for model_type in df_bestfit['agent_type'].drop_duplicates():\n",
    "    # get only the rows of the best-fit dataframe for the current model type\n",
    "    df_model = df_bestfit[df_bestfit['agent_type'] == model_type]\n",
    "    print(f\"MODEL TYPE: {model_type}\")\n",
    "    print(f\"exp param: {df_model['exp_param'].mean()}\")\n",
    "    print(f\"vm param: {df_model['vm_param'].mean()}\")\n",
    "    if model_type in (\"hill_climbing\", \"sparse_max_continuous\", \"sparse_max_discrete\"):\n",
    "        print(f\"step size: {df_model['step_size'].mean()}\")\n",
    "    if model_type in (\"sparse_lqr\", \"sparse_max_continuous\", \"sparse_max_discrete\"):\n",
    "        print(f\"attention cost: {df_model['attention_cost'].mean()}\")\n",
    "    if model_type == \"null_model_1\":  # n and b parameters for nm1\n",
    "        print(f\"n: {np.round(df_model['n']).mean()}\")\n",
    "        print(f\"b: {df_model['b'].mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Participant Scores by Best-Fitting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the participant data\n",
    "raw_pp_data_path = '../../data/experimental_data/experiment_actions.csv'\n",
    "df_pps = pd.read_csv(raw_pp_data_path)\n",
    "df_last = df_pps.loc[df_pps.groupby(\"pp_id\")['Unnamed: 0'].idxmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get scores by which model explains each pp best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_by_best_model = defaultdict(list)\n",
    "for index, row in df_last.iterrows():\n",
    "    scores_by_best_model[participant_to_best_model[row['pp_id']]].append(np.sqrt(row['total_cost']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse lqr median: 95.97784526310616\n",
      "hill climbing median: 114.59424069297725\n"
     ]
    }
   ],
   "source": [
    "print(f\"sparse lqr median: {np.median(scores_by_best_model['sparse_lqr'])}\")\n",
    "print(f\"hill climbing median: {np.median(scores_by_best_model['hill_climbing'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_ci(data, n=1000000):\n",
    "    all_medians = []\n",
    "    for i in range(n):\n",
    "        bs_data = np.random.choice(data, len(data), replace=True)\n",
    "        med = np.median(bs_data)\n",
    "        all_medians.append(med)\n",
    "    all_medians = np.array(all_medians)\n",
    "    lower_bound = np.percentile(all_medians, 2.5)\n",
    "    upper_bound = np.percentile(all_medians, 97.5)\n",
    "    \n",
    "    return lower_bound, upper_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse_lqr: 95.97784526310616, [27.803339825313234, 172.17916861758323]\n",
      "hill_climbing: 114.59424069297725, [96.429404229208, 127.38928526371438]\n"
     ]
    }
   ],
   "source": [
    "for model_type in (\"sparse_lqr\", \"hill_climbing\"):\n",
    "    scores = scores_by_best_model[model_type]\n",
    "    lower_bound, upper_bound = bootstrap_ci(scores)\n",
    "    print(f\"{model_type}: {np.median(scores)}, [{lower_bound}, {upper_bound}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KruskalResult(statistic=0.5535135135135079, pvalue=0.45688561764287994)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.kruskal(scores_by_best_model['sparse_lqr'], scores_by_best_model['hill_climbing'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get descriptive stats like number of variables manipulated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    }
   ],
   "source": [
    "print(len(scores_by_best_model['sparse_lqr']) + len(scores_by_best_model['hill_climbing']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the number of variables manipulated and input norm standard deviation for humans."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
