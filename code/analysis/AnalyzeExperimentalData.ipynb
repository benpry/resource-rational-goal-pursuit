{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from ast import literal_eval\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read each fitting results file and print the fitting criteria and mean parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../../data/fitting_results\"\n",
    "FIGURE_DIR = \"../../figures\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent type: lqr, n=111\n",
      "lqr llh: -13064.787564000002\n",
      "lqr AIC: 26573.575128000004\n",
      "lqr llh mean: -117.70078886486486\n",
      "lqr AIC mean: 239.40157772972967\n",
      "mean exp param: 0.009439116295392144\n",
      "mean vm param: 1.7997701788396911\n",
      "agent type: sparse_lqr, n=111\n",
      "sparse_lqr llh: -9031.817217\n",
      "sparse_lqr AIC: 18729.634434\n",
      "sparse_lqr llh mean: -81.36772267567568\n",
      "sparse_lqr AIC mean: 168.7354453513514\n",
      "agent type: sparse_max_discrete, n=111\n",
      "sparse_max_discrete llh: -8190.639816\n",
      "sparse_max_discrete AIC: 17269.279632\n",
      "sparse_max_discrete llh mean: -73.78954789189193\n",
      "sparse_max_discrete AIC mean: 155.57909578378386\n",
      "agent type: sparse_max_continuous, n=111\n",
      "sparse_max_continuous llh: -8364.688535000001\n",
      "sparse_max_continuous AIC: 17617.377069999995\n",
      "sparse_max_continuous llh mean: -75.35755436936938\n",
      "sparse_max_continuous AIC mean: 158.7151087387387\n",
      "agent type: null_model_1, n=111\n",
      "null_model_1 llh: -8834.4462921\n",
      "null_model_1 AIC: 18556.8925842\n",
      "null_model_1 llh mean: -79.58960623513514\n",
      "null_model_1 AIC mean: 167.1792124702703\n",
      "mean exp param: 0.037384772569791704\n",
      "mean vm param: 4.3725591142940985\n",
      "mean n: 2.3703191025998387\n",
      "median n: 1.9735883630406712\n",
      "mean b: 2.7923860496732975\n",
      "agent type: null_model_2, n=111\n",
      "null_model_2 llh: -8720.065119\n",
      "null_model_2 AIC: 17884.130238\n",
      "null_model_2 llh mean: -78.55914521621622\n",
      "null_model_2 AIC mean: 161.11829043243242\n",
      "agent type: hill_climbing, n=111\n",
      "hill_climbing llh: -8324.572544\n",
      "hill_climbing AIC: 17315.145088\n",
      "hill_climbing llh mean: -74.99614904504504\n",
      "hill_climbing AIC mean: 155.99229809009006\n"
     ]
    }
   ],
   "source": [
    "# create a dictionary to store dataframes\n",
    "agent_types = ['lqr', 'sparse_lqr', 'sparse_max_discrete', 'sparse_max_continuous', 'null_model_1', 'null_model_2', 'hill_climbing']\n",
    "all_model_dfs = {}\n",
    "for agent_type in agent_types:\n",
    "    # read the aggregated fitting results\n",
    "    df = pd.read_csv(f\"{DATA_DIR}/fitting_results_{agent_type}.csv\")\n",
    "    all_model_dfs[agent_type] = df\n",
    "    # print info about \n",
    "    print(f\"agent type: {agent_type}, n={len(df)}\")\n",
    "    print(f\"{agent_type} llh: {df['ll'].sum()}\")\n",
    "    print(f\"{agent_type} AIC: {df['AIC'].sum()}\")\n",
    "    print(f\"{agent_type} llh mean: {df['ll'].mean()}\")\n",
    "    print(f\"{agent_type} AIC mean: {df['AIC'].mean()}\")\n",
    "    if agent_type == \"lqr\" or agent_type == \"null_model_1\":\n",
    "        print(f\"mean exp param: {df['exp_param'].mean()}\")\n",
    "        print(f\"mean vm param: {df['vm_param'].mean()}\")\n",
    "    if agent_type == \"null_model_1\":\n",
    "        print(f\"mean n: {df['n'].mean()}\")\n",
    "        print(f\"median n: {df['n'].median()}\")\n",
    "        print(f\"mean b: {df['b'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the participant ids\n",
    "pp_nrs = pd.read_csv('../../data/experimental_data/experiment_ppids.csv')['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the number of participants best fit by each model, as well as the strength of evidence for the best model over the second-best model for each participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'null_model_2': 16, 'hill_climbing': 37, 'sparse_max_discrete': 32, 'sparse_max_continuous': 14, 'sparse_lqr': 12})\n"
     ]
    }
   ],
   "source": [
    "models_by_best_fitting_pps = defaultdict(int)\n",
    "participant_to_best_model = {}\n",
    "nm1_diff_by_pp = {}\n",
    "\n",
    "for pp_id in pp_nrs:\n",
    "    participant_fits = {}\n",
    "    for agent_type in agent_types:\n",
    "        # select the dataframe for the selected agent type\n",
    "        df = all_model_dfs[agent_type]\n",
    "\n",
    "        if len(df[df['pp_id'] == pp_id]['AIC']) != 1:\n",
    "            print(\"len:\", len(df[df['pp_id'] == pp_id]['AIC']))\n",
    "            break\n",
    "        \n",
    "        # add the AIC to the participant fits dictionary\n",
    "        participant_fits[agent_type] = float(df[df['pp_id'] == pp_id]['AIC'])\n",
    "    \n",
    "    if len(df[df['pp_id'] == pp_id]) != 1:\n",
    "        continue\n",
    "    \n",
    "    # select the best-fitting agent for this participant\n",
    "    sorted_fits = sorted(participant_fits.values())\n",
    "    best_agent = min(participant_fits, key=participant_fits.get)\n",
    "    nm1_diff =  np.mean([participant_fits[at] for at in agent_types if at != \"null_model_1\"]) - participant_fits[\"null_model_1\"]\n",
    "    \n",
    "    # increment the number of pps best fit by the model\n",
    "    models_by_best_fitting_pps[best_agent] += 1\n",
    "    # store the best-fitting model for this participant\n",
    "    participant_to_best_model[pp_id] = best_agent\n",
    "    # save the difference between nm1 and the second best-fitting model\n",
    "    nm1_diff_by_pp[pp_id] = nm1_diff\n",
    "\n",
    "print(models_by_best_fitting_pps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the top 10 participants by nm1's fit\n",
    "best_pps_nm1 = sorted(nm1_diff_by_pp, key=nm1_diff_by_pp.get, reverse=True)[:10]\n",
    "df_nm1 = all_model_dfs[\"null_model_1\"]\n",
    "df_nm1 = df_nm1[df_nm1[\"pp_id\"].isin(best_pps_nm1)]\n",
    "df_nm1.to_csv(\"../../data/fitting_results/nm1_best_pps.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create csv files for Bayesian model selection (done using SPM8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aics = pd.DataFrame()\n",
    "for df_type in all_model_dfs:\n",
    "    # convert AICs to log model evidence format\n",
    "    df_aics[df_type] = all_model_dfs[df_type]['AIC'].apply(lambda x: -x/2)\n",
    "df_aics.to_csv(f\"{DATA_DIR}/aic_lme.csv\")  # save to csv\n",
    "\n",
    "n_params = {\"null_model_2\": 2, \"null_model_1\": 4, \"lqr\": 2, \"sparse_lqr\": 3, \"hill_climbing\": 3, \"sparse_max_continuous\": 4, \"sparse_max_discrete\": 4}\n",
    "\n",
    "df_bics = pd.DataFrame()\n",
    "for df_type in all_model_dfs:\n",
    "    # convert BICs to log model evidence format\n",
    "    df_bics[df_type] = all_model_dfs[df_type][\"ll\"].apply(lambda x: - (n_params[df_type] * np.log(10) - 2 * x) / 2)\n",
    "df_bics.to_csv(f\"{DATA_DIR}/bic_lme.csv\")  # save to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a csv file with the best-fitting model and parameters for each participant\n",
    "df_bestfit = pd.DataFrame()\n",
    "for pp_id in pp_nrs:\n",
    "    best_model_type = participant_to_best_model[pp_id]  # pick the best model type\n",
    "    df = all_model_dfs[best_model_type]  # get the dataframe for the best model\n",
    "    row = df[df['pp_id'] == pp_id]  # get the relevant row using the pp id\n",
    "    df_bestfit = df_bestfit.append(row, ignore_index=True)  # add the selected row to the best fit dataframe\n",
    "df_bestfit.to_csv(f\"{DATA_DIR}/best_fitting_models.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the mean best-fitting parameter for each model type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL TYPE: null_model_2\n",
      "exp param: 0.0266682604176051\n",
      "vm param: 6.083610428849356\n",
      "MODEL TYPE: hill_climbing\n",
      "exp param: 0.07337562077381873\n",
      "vm param: 6.0093102211512015\n",
      "step size: 0.33126701214033644\n",
      "MODEL TYPE: sparse_max_discrete\n",
      "exp param: 0.10871181966031433\n",
      "vm param: 4.5227043630257295\n",
      "step size: 0.6898139999425017\n",
      "attention cost: 14.235119906069837\n",
      "MODEL TYPE: sparse_max_continuous\n",
      "exp param: 0.03556975179584271\n",
      "vm param: 4.973135682639295\n",
      "step size: 0.2719986741210881\n",
      "attention cost: 12.254898554996227\n",
      "MODEL TYPE: sparse_lqr\n",
      "exp param: 0.05874843995317245\n",
      "vm param: 4.40096609684599\n",
      "attention cost: 150.69867445912084\n"
     ]
    }
   ],
   "source": [
    "for model_type in df_bestfit['agent_type'].drop_duplicates():\n",
    "    # get only the rows of the best-fit dataframe for the current model type\n",
    "    df_model = df_bestfit[df_bestfit['agent_type'] == model_type]\n",
    "    print(f\"MODEL TYPE: {model_type}\")\n",
    "    print(f\"exp param: {df_model['exp_param'].mean()}\")\n",
    "    print(f\"vm param: {df_model['vm_param'].mean()}\")\n",
    "    if model_type in (\"hill_climbing\", \"sparse_max_continuous\", \"sparse_max_discrete\"):\n",
    "        print(f\"step size: {df_model['step_size'].mean()}\")\n",
    "    if model_type in (\"sparse_lqr\", \"sparse_max_continuous\", \"sparse_max_discrete\"):\n",
    "        print(f\"attention cost: {df_model['attention_cost'].mean()}\")\n",
    "    if model_type == \"null_model_1\":  # n and b parameters for nm1\n",
    "        print(f\"n: {np.round(df_model['n']).mean()}\")\n",
    "        print(f\"b: {df_model['b'].mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Participant Scores by Best-Fitting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the participant data\n",
    "raw_pp_data_path = '../../data/experimental_data/experiment_actions.csv'\n",
    "df_pps = pd.read_csv(raw_pp_data_path)\n",
    "df_last = df_pps.loc[df_pps.groupby(\"pp_id\")['Unnamed: 0'].idxmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get scores by which model explains each pp best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_by_best_model = defaultdict(list)\n",
    "for index, row in df_last.iterrows():\n",
    "    scores_by_best_model[participant_to_best_model[row['pp_id']]].append(np.sqrt(row['total_cost']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse lqr median: 95.97784526310616\n",
      "hill climbing median: 114.59424069297725\n"
     ]
    }
   ],
   "source": [
    "print(f\"sparse lqr median: {np.median(scores_by_best_model['sparse_lqr'])}\")\n",
    "print(f\"hill climbing median: {np.median(scores_by_best_model['hill_climbing'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_ci(data, n=1000000):\n",
    "    all_medians = []\n",
    "    for i in range(n):\n",
    "        bs_data = np.random.choice(data, len(data), replace=True)\n",
    "        med = np.median(bs_data)\n",
    "        all_medians.append(med)\n",
    "    all_medians = np.array(all_medians)\n",
    "    lower_bound = np.percentile(all_medians, 2.5)\n",
    "    upper_bound = np.percentile(all_medians, 97.5)\n",
    "    \n",
    "    return lower_bound, upper_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse_lqr: 95.97784526310616, [30.046612351523706, 150.7278131887344]\n",
      "hill_climbing: 114.59424069297725, [96.429404229208, 129.1154522123514]\n"
     ]
    }
   ],
   "source": [
    "for model_type in (\"sparse_lqr\", \"hill_climbing\"):\n",
    "    scores = scores_by_best_model[model_type]\n",
    "    lower_bound, upper_bound = bootstrap_ci(scores)\n",
    "    print(f\"{model_type}: {np.median(scores)}, [{lower_bound}, {upper_bound}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KruskalResult(statistic=0.6248648648648611, pvalue=0.42924519572005637)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.kruskal(scores_by_best_model['sparse_lqr'], scores_by_best_model['hill_climbing'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get descriptive stats like number of variables manipulated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    }
   ],
   "source": [
    "print(len(scores_by_best_model['sparse_lqr']) + len(scores_by_best_model['hill_climbing']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the number of variables manipulated and input norm standard deviation for humans."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
