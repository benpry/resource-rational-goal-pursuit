{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from ast import literal_eval\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read each fitting results file and print the fitting criteria and mean parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../../data/fitting_results\"\n",
    "FIGURE_DIR = \"../../figures\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent type: lqr, n=111\n",
      "lqr llh: -12814.148508\n",
      "lqr AIC: 26072.297016\n",
      "lqr llh mean: -115.44277935135132\n",
      "lqr AIC mean: 234.8855587027027\n",
      "mean exp param: 0.00940909749817091\n",
      "mean vm param: 2.020889219661038\n",
      "agent type: sparse_lqr, n=111\n",
      "sparse_lqr llh: -8979.656426999998\n",
      "sparse_lqr AIC: 18625.312853999996\n",
      "sparse_lqr llh mean: -80.89780564864864\n",
      "sparse_lqr AIC mean: 167.79561129729726\n",
      "agent type: sparse_max_discrete, n=111\n",
      "sparse_max_discrete llh: -8207.513571\n",
      "sparse_max_discrete AIC: 17303.027142\n",
      "sparse_max_discrete llh mean: -73.9415637027027\n",
      "sparse_max_discrete AIC mean: 155.8831274054054\n",
      "agent type: sparse_max_continuous, n=111\n",
      "sparse_max_continuous llh: -8387.806881\n",
      "sparse_max_continuous AIC: 17663.613762\n",
      "sparse_max_continuous llh mean: -75.56582775675678\n",
      "sparse_max_continuous AIC mean: 159.13165551351355\n",
      "agent type: null_model_1, n=111\n",
      "null_model_1 llh: -8869.6115108\n",
      "null_model_1 AIC: 18627.2230216\n",
      "null_model_1 llh mean: -79.9064100072072\n",
      "null_model_1 AIC mean: 167.81282001441443\n",
      "agent type: null_model_2, n=111\n",
      "null_model_2 llh: -8720.326137\n",
      "null_model_2 AIC: 17884.652274\n",
      "null_model_2 llh mean: -78.56149672972973\n",
      "null_model_2 AIC mean: 161.12299345945942\n",
      "agent type: hill_climbing, n=111\n",
      "hill_climbing llh: -8315.129718\n",
      "hill_climbing AIC: 17296.259436\n",
      "hill_climbing llh mean: -74.91107854054057\n",
      "hill_climbing AIC mean: 155.82215708108114\n"
     ]
    }
   ],
   "source": [
    "# create a dictionary to store dataframes\n",
    "all_model_dfs = {}\n",
    "for agent_type in ['lqr', 'sparse_lqr', 'sparse_max_discrete', 'sparse_max_continuous', 'null_model_1', 'null_model_2', 'hill_climbing']:\n",
    "    # read the aggregated fitting results\n",
    "    df = pd.read_csv(f\"{DATA_DIR}/fitting_results_{agent_type}.csv\")\n",
    "    all_model_dfs[agent_type] = df\n",
    "    # print info about \n",
    "    print(f\"agent type: {agent_type}, n={len(df)}\")\n",
    "    print(f\"{agent_type} llh: {df['ll'].sum()}\")\n",
    "    print(f\"{agent_type} AIC: {df['AIC'].sum()}\")\n",
    "    print(f\"{agent_type} llh mean: {df['ll'].mean()}\")\n",
    "    print(f\"{agent_type} AIC mean: {df['AIC'].mean()}\")\n",
    "    if agent_type == \"lqr\":\n",
    "        print(f\"mean exp param: {df['exp_param'].mean()}\")\n",
    "        print(f\"mean vm param: {df['vm_param'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the participant ids\n",
    "pp_nrs = pd.read_csv('../../data/experimental_data/experiment_ppids.csv')['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the number of participants best fit by each model, as well as the strength of evidence for the best model over the second-best model for each participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'null_model_2': 17, 'hill_climbing': 35, 'sparse_max_discrete': 33, 'sparse_lqr': 15, 'sparse_max_continuous': 10, 'lqr': 1})\n"
     ]
    }
   ],
   "source": [
    "models_by_best_fitting_pps = defaultdict(int)\n",
    "participant_to_best_model = {}\n",
    "\n",
    "for pp_id in pp_nrs:\n",
    "    participant_fits = {}\n",
    "    for agent_type in ['lqr', 'sparse_lqr', 'sparse_max_discrete', 'sparse_max_continuous', 'null_model_1', 'null_model_2', 'hill_climbing']:\n",
    "        # select the dataframe for the selected agent type\n",
    "        df = all_model_dfs[agent_type]\n",
    "\n",
    "        if len(df[df['pp_id'] == pp_id]['AIC']) != 1:\n",
    "            print(\"len:\", len(df[df['pp_id'] == pp_id]['AIC']))\n",
    "            break\n",
    "        \n",
    "        # add the AIC to the participant fits dictionary\n",
    "        participant_fits[agent_type] = float(df[df['pp_id'] == pp_id]['AIC'])\n",
    "    \n",
    "    if len(df[df['pp_id'] == pp_id]) != 1:\n",
    "        continue\n",
    "    \n",
    "    # select the best-fitting agent for this participant\n",
    "    sorted_fits = sorted(participant_fits.values())\n",
    "    best_agent = min(participant_fits, key=participant_fits.get)\n",
    "    \n",
    "    # increment the number of pps best fit by the model\n",
    "    models_by_best_fitting_pps[best_agent] += 1\n",
    "    # store the best-fitting model for this participant\n",
    "    participant_to_best_model[pp_id] = best_agent\n",
    "\n",
    "print(models_by_best_fitting_pps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create csv files for Bayesian model selection (done using SPM8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aics = pd.DataFrame()\n",
    "for df_type in all_model_dfs:\n",
    "    # convert AICs to log model evidence format\n",
    "    df_aics[df_type] = all_model_dfs[df_type]['AIC'].apply(lambda x: -x/2)\n",
    "df_aics.to_csv(f\"{DATA_DIR}/aic_lme.csv\")  # save to csv\n",
    "\n",
    "n_params = {\"null_model_2\": 2, \"null_model_1\": 4, \"lqr\": 2, \"sparse_lqr\": 3, \"hill_climbing\": 3, \"sparse_max_continuous\": 4, \"sparse_max_discrete\": 4}\n",
    "\n",
    "df_bics = pd.DataFrame()\n",
    "for df_type in all_model_dfs:\n",
    "    # convert BICs to log model evidence format\n",
    "    df_bics[df_type] = all_model_dfs[df_type][\"ll\"].apply(lambda x: - (n_params[df_type] * np.log(10) - 2 * x) / 2)\n",
    "df_bics.to_csv(f\"{DATA_DIR}/bic_lme.csv\")  # save to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a csv file with the best-fitting model and parameters for each participant\n",
    "df_bestfit = pd.DataFrame()\n",
    "for pp_id in pp_nrs:\n",
    "    best_model_type = participant_to_best_model[pp_id]  # pick the best model type\n",
    "    df = all_model_dfs[best_model_type]  # get the dataframe for the best model\n",
    "    row = df[df['pp_id'] == pp_id]  # get the relevant row using the pp id\n",
    "    df_bestfit = df_bestfit.append(row, ignore_index=True)  # add the selected row to the best fit dataframe\n",
    "df_bestfit.to_csv(f\"{DATA_DIR}/best_fitting_models.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the mean best-fitting parameter for each model type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL TYPE: null_model_2\n",
      "exp param: 0.02613745480687401\n",
      "vm param: 5.610590213263846\n",
      "MODEL TYPE: hill_climbing\n",
      "exp param: 0.06961741245647332\n",
      "vm param: 5.888150693552178\n",
      "step size: 0.3845915454380134\n",
      "MODEL TYPE: sparse_max_discrete\n",
      "exp param: 0.09070498285942245\n",
      "vm param: 4.406183043702149\n",
      "step size: 0.6362588893687267\n",
      "attention cost: 12.433202622236541\n",
      "MODEL TYPE: sparse_lqr\n",
      "exp param: 0.051042430933889826\n",
      "vm param: 4.445905714461947\n",
      "attention cost: 136.06803730850126\n",
      "MODEL TYPE: sparse_max_continuous\n",
      "exp param: 0.05548931159039938\n",
      "vm param: 5.265772347464798\n",
      "step size: 0.2071716908934227\n",
      "attention cost: 14.49441125346252\n",
      "MODEL TYPE: lqr\n",
      "exp param: 0.025278183757093\n",
      "vm param: 4.268855736237463\n"
     ]
    }
   ],
   "source": [
    "for model_type in df_bestfit['agent_type'].drop_duplicates():\n",
    "    # get only the rows of the best-fit dataframe for the current model type\n",
    "    df_model = df_bestfit[df_bestfit['agent_type'] == model_type]\n",
    "    print(f\"MODEL TYPE: {model_type}\")\n",
    "    print(f\"exp param: {df_model['exp_param'].mean()}\")\n",
    "    print(f\"vm param: {df_model['vm_param'].mean()}\")\n",
    "    if model_type in (\"hill_climbing\", \"sparse_max_continuous\", \"sparse_max_discrete\"):\n",
    "        print(f\"step size: {df_model['step_size'].mean()}\")\n",
    "    if model_type in (\"sparse_lqr\", \"sparse_max_continuous\", \"sparse_max_discrete\"):\n",
    "        print(f\"attention cost: {df_model['attention_cost'].mean()}\")\n",
    "    if model_type == \"null_model_1\":  # n and b parameters for nm1\n",
    "        print(f\"n: {np.round(df_model['n']).mean()}\")\n",
    "        print(f\"b: {df_model['b'].mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Participant Scores by Best-Fitting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the participant data\n",
    "raw_pp_data_path = '../../data/experimental_data/experiment_actions.csv'\n",
    "df_pps = pd.read_csv(raw_pp_data_path)\n",
    "df_last = df_pps.loc[df_pps.groupby(\"pp_id\")['Unnamed: 0'].idxmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get scores by which model explains each pp best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_by_best_model = defaultdict(list)\n",
    "for index, row in df_last.iterrows():\n",
    "    scores_by_best_model[participant_to_best_model[row['pp_id']]].append(np.sqrt(row['total_cost']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse lqr median: 90.62030677502698\n",
      "hill climbing median: 115.66256957200979\n"
     ]
    }
   ],
   "source": [
    "print(f\"sparse lqr median: {np.median(scores_by_best_model['sparse_lqr'])}\")\n",
    "print(f\"hill climbing median: {np.median(scores_by_best_model['hill_climbing'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_ci(data, n=1000000):\n",
    "    all_medians = []\n",
    "    for i in range(n):\n",
    "        bs_data = np.random.choice(data, len(data), replace=True)\n",
    "        med = np.median(bs_data)\n",
    "        all_medians.append(med)\n",
    "    all_medians = np.array(all_medians)\n",
    "    lower_bound = np.percentile(all_medians, 2.5)\n",
    "    upper_bound = np.percentile(all_medians, 97.5)\n",
    "    \n",
    "    return lower_bound, upper_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse_lqr: 90.62030677502698, [26.370058778849923, 172.33374596984768]\n",
      "hill_climbing: 115.66256957200979, [99.15694630231407, 129.1154522123514]\n"
     ]
    }
   ],
   "source": [
    "for model_type in (\"sparse_lqr\", \"hill_climbing\"):\n",
    "    scores = scores_by_best_model[model_type]\n",
    "    lower_bound, upper_bound = bootstrap_ci(scores)\n",
    "    print(f\"{model_type}: {np.median(scores)}, [{lower_bound}, {upper_bound}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KruskalResult(statistic=1.0112044817927028, pvalue=0.3146144552999878)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.kruskal(scores_by_best_model['sparse_lqr'], scores_by_best_model['hill_climbing'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get descriptive stats like number of variables manipulated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "print(len(scores_by_best_model['sparse_lqr']) + len(scores_by_best_model['hill_climbing']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the number of variables manipulated and input norm standard deviation for humans."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
