{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "from scipy.stats import kruskal, iqr\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lqr_file = \"../../data/qualitative_data/lqr_all_situations.csv\"\n",
    "pp_data_file = '../../data/experimental_data/experiment_actions.csv'\n",
    "model_score_file = \"../../data/qualitative_data/all_model_runs_on_situations_canonical.csv\"\n",
    "conditions_file = \"../../data/experimental_data/experiment_conditions.csv\"\n",
    "n_pps = 111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all the data\n",
    "df_data = pd.read_csv(pp_data_file)\n",
    "df_pps = df_data.loc[df_data.groupby('pp_id')['id'].idxmax()]\n",
    "df_lqr = pd.read_csv(lqr_file)\n",
    "df_model_scores = pd.read_csv(model_score_file)\n",
    "df_conditions = pd.read_csv(conditions_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [241.0, -127.0, -192.0, 113.0, -224.0]\n",
       "1        [95.0, -108.0, -140.0, 62.0, -173.0]\n",
       "2         [-84.0, 219.0, 125.0, -84.0, 132.0]\n",
       "3             [-88.0, 3.0, 58.0, 82.0, 231.0]\n",
       "4             [-32.0, 39.0, 75.0, 175.0, 6.0]\n",
       "                        ...                  \n",
       "135        [-40.0, -43.0, -35.0, 199.0, 67.0]\n",
       "136       [-184.0, 235.0, 38.0, -237.0, 65.0]\n",
       "137      [-122.0, 71.0, -217.0, 131.0, 165.0]\n",
       "138         [106.0, 40.0, -10.0, -191.0, 3.0]\n",
       "139          [156.0, 99.0, 38.0, -50.0, 56.0]\n",
       "Name: initial_endogenous, Length: 140, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_conditions[\"initial_endogenous\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the situations to string\n",
    "df_conditions['initial_endogenous'] = df_conditions['initial_endogenous'].apply(lambda x: str([int(y) for y in literal_eval(x)]))\n",
    "# drop all but the situation and lqr score columns\n",
    "columns_to_keep = ['situation', 'lqr_score']\n",
    "df_lqr = df_lqr.merge(df_conditions, how='left', left_on='situation', right_on='initial_endogenous')[columns_to_keep]\n",
    "df_by_pp = df_data.groupby('pp_id').idxmax()  # dataframe with one row per participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of people assigned to the easy and informative conditions\n",
    "n_easy = 0\n",
    "n_info = 0\n",
    "for c in df_by_pp['condition']:\n",
    "    cond = c % 30\n",
    "    if cond < 10:\n",
    "        n_easy += 1\n",
    "    else:\n",
    "        n_info += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116.24646231176244"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# median human performance\n",
    "df_pps['final_goal_distance'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of participants who got within 100 points of the goal\n",
    "len(df_pps[df_pps['final_goal_distance'] < 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the root costs\n",
    "df_pps['root_cost'] = df_pps['total_cost'].apply(np.sqrt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the mean and median costs and root costs for each model type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "costs = defaultdict(list)\n",
    "# create lists of all the scores achieved by each agent type\n",
    "for index, row in df_pps.iterrows():\n",
    "    # add the human score\n",
    "    costs['human'].append(np.sqrt(row['total_cost']))\n",
    "    # add the lqr score\n",
    "    condition = int(row['condition'])\n",
    "    costs['lqr'].append(df_lqr.loc[condition % 30]['lqr_score'])\n",
    "    \n",
    "# compute the mean and median costs (means get skewed by a few outliers. Medians are more meaningful.)\n",
    "avg_costs, med_costs = {}, {}\n",
    "for score_type in costs:\n",
    "    avg_costs[score_type] = np.mean(costs[score_type])\n",
    "    med_costs[score_type] = np.median(costs[score_type])\n",
    "for agent_type in df_model_scores[\"model\"].drop_duplicates():\n",
    "    avg_costs[agent_type] = df_model_scores[df_model_scores[\"model\"] == agent_type][\"performance\"].mean()\n",
    "    med_costs[agent_type] = df_model_scores[df_model_scores[\"model\"] == agent_type][\"performance\"].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'human': 1033.664207131002,\n",
       " 'lqr': 6.706318696972033,\n",
       " 'null_model_2': 629.6316683697174,\n",
       " 'hill_climbing': 166.41983635879137,\n",
       " 'sparse_max_discrete': 144.8840354815775,\n",
       " 'sparse_lqr': 143.5613097157767,\n",
       " 'sparse_max_continuous': 357.4669374598366,\n",
       " 'null_model_1': 611.7676810539906}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'human': 116.42950656942597,\n",
       " 'lqr': 6.374889832582812,\n",
       " 'null_model_2': 505.3941210937507,\n",
       " 'hill_climbing': 138.05166333818937,\n",
       " 'sparse_max_discrete': 119.36807647547414,\n",
       " 'sparse_lqr': 83.78617356356702,\n",
       " 'sparse_max_continuous': 263.7716172144445,\n",
       " 'null_model_1': 492.8503046075971}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "med_costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "241.03226556653763\n",
      "116.42950656942597\n",
      "2440.5859034079267\n",
      "117.29339106392572\n"
     ]
    }
   ],
   "source": [
    "df_pp_condition = df_pps.merge(df_conditions, left_on='condition', right_on='goal_id')  # merge the condition labels (easy, hard) with the pp data\n",
    "df_pp_condition['root_cost'] = df_pp_condition['total_cost'].apply(np.sqrt)  # compute the root costs\n",
    "# print the mean and median score of people in each condition\n",
    "print(df_pp_condition[df_pp_condition['conditions'] == 'informative']['root_cost'].mean())\n",
    "print(df_pp_condition[df_pp_condition['conditions'] == 'informative']['root_cost'].median())\n",
    "print(df_pp_condition[df_pp_condition['conditions'] == 'easy']['root_cost'].mean())\n",
    "print(df_pp_condition[df_pp_condition['conditions'] == 'easy']['root_cost'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pps['condition_name'] = df_pps['condition'].apply(lambda x: \"informative\" if x % 30 >= 10 else \"easy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the median human score between easy and informative conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "easy_costs = df_pps[df_pps['condition_name'] == 'easy']['root_cost']\n",
    "informative_costs = df_pps[df_pps['condition_name'] == 'informative']['root_cost']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "easy median: 117.29339106392572\n",
      "informative median: 116.42950656942597\n"
     ]
    }
   ],
   "source": [
    "print(f\"easy median: {easy_costs.median()}\")\n",
    "print(f\"informative median: {informative_costs.median()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KruskalResult(statistic=0.0006036217304199454, pvalue=0.9803989740043368)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kruskal(easy_costs, informative_costs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
